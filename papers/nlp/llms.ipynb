{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b1d564",
   "metadata": {},
   "source": [
    "# Natural Language Processing: Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da3d79",
   "metadata": {},
   "source": [
    "* [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)\n",
    "    - Overview: In this paper, we take a step back and ask:\n",
    "        How big is too big? What are the possible risks associated with this\n",
    "        technology and what paths are available for mitigating those risks?\n",
    "        We provide recommendations including weighing the environmental and      financial costs first, investing resources into curating and\n",
    "        carefully documenting datasets rather than ingesting everything on\n",
    "        the web, carrying out pre-development exercises evaluating how\n",
    "        the planned approach fits into research and development goals and\n",
    "        supports stakeholder values, and encouraging research directions\n",
    "        beyond ever larger language models.\n",
    "    - Miscellaneous: [Was Homer a stochastic parrot? Meaning in literary texts and LLMs](https://www.lesswrong.com/posts/EREcbR5jiLvdPcSB3/was-homer-a-stochastic-parrot-meaning-in-literary-texts-and)\n",
    "    \n",
    "* [No Language Left Behind: Scaling Human-Centered Machine Translation](https://paperswithcode.com/paper/no-language-left-behind-scaling-human-1)\n",
    "    - Overview: Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages.\n",
    "\n",
    "* [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085v1.pdf)\n",
    "    - Overview: Information overload is a major obstacle to scientific progress. The explosive growth inscientific literature and data has made it ever harder to discover useful insights in a largemass of information. Today scientific knowledge is accessed through search engines, butthey are unable to organize scientific knowledge alone. In this paper we introduce Galactica:a large language model that can store, combine and reason about scientific knowledge. Wetrain on a large scientific corpus of papers, reference material, knowledge bases and manyother sources. We outperform existing models on a range of scientific tasks. \n",
    "    \n",
    "* [Situating Search](https://dl.acm.org/doi/pdf/10.1145/3498366.3505816)\n",
    "    - Overview: Search systems, like many other applications of machine learning,have become increasingly complex and opaque. The notions of relevance, usefulness, and trustworthiness with respect to information were already overloaded and often difficult to articulate, study, or implement. Newly surfaced proposals that aim to use large language models to generate relevant information for a userâ€™s needs pose even greater threat to transparency, provenance, and user interactions in a search system. In this perspective paper we revisit the problem of search in the larger context of information seeking and argue that removing or reducing interactions in an effort to retrieve presumably more relevant information can be detrimental to many fundamental aspects of search, including information verification, information literacy, and serendipity. In addition to providing suggestions for counteracting some of the potential problems posed by such models, we present a vision for search systems that are intelligent and effective, while also providing greater transparency and accountability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
