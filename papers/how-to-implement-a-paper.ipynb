{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec6e776",
   "metadata": {},
   "source": [
    "# Tips on how to implement a paper in PyTorch: \n",
    "\n",
    "\n",
    "1) [PyTorch Paper Replicating from Zero to Mastery Learn PyTorch for Deep Learning](https://www.learnpytorch.io/08_pytorch_paper_replicating/)\n",
    "    - Note: \n",
    "        - Paper: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (the paper that introduced the ViT archiecture)](https://arxiv.org/pdf/2010.11929.pdf)\n",
    "        - [YouTube Video](https://www.learnpytorch.io/08_pytorch_paper_replicating/)\n",
    "        \n",
    "2) Misclellanous: \n",
    "    * [Learn To Reproduce Papers: Beginner's Guide](https://towardsdatascience.com/learn-to-reproduce-papers-beginners-guide-2b4bff8fcca0)\n",
    "    * [Tips on how my Git repo will look when I start implementing papers](https://github.com/jaygala24/pytorch-implementations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e660eb",
   "metadata": {},
   "source": [
    "We're going to focus on recreating the Vision Transformer (ViT) computer vision architecture and applying it to our FoodVisoin Mini problem to classify different images of pizza, steak and sushi. \n",
    "\n",
    "The goal of __replicating or implementing__ a machine learning paper is apply the most recent techniques to your own problems. So it is indeed a valuable skill to have. \n",
    "\n",
    "In other words, ML paper replicating involves turning an ML paper comprised of images/diagrams, math and text into ussable code in our case, usable PyTorch code.\n",
    "\n",
    "A machine learning paper is usually divided in the following parts: \n",
    "\n",
    "* __Abstract__: An overview/summary of the paper's main findings/contributions.\n",
    "* __Introduction__: This section answers the following questions: \n",
    "    - What's the paper's main problem and details of previous methods used to try and solve it?\n",
    "* __Method__: This section answers the following questions: \n",
    "    - How did the researchers go about conducting their research? \n",
    "    - e.g, What model(s), data sources, training setups were used? \n",
    "* __Results__: This section answers the following questions: \n",
    "    - What are the outcomes of the paper? \n",
    "    - e.g, If a new model or training setup was used, how did the results of findings compare to previous works? (this is where _experiment tracking_ comes in handy)\n",
    "* __Conclusion__: This section answers the following questions: \n",
    "    - What are the limitations of the suggested methods? \n",
    "    - What are some next steps for the research community?\n",
    "* __References__: This sections answers the following questions: \n",
    "    - What resources/other papers did the researchers look at to build their own body of work? \n",
    "* __Appendix__: \n",
    "    - Are there any extra resources/findings to look at that weren't included in any of the above sections?\n",
    "\n",
    "\n",
    "__Machine Learning Engineer(Motto)__: \n",
    "\n",
    "1) Download a paper\n",
    "2) Implement it\n",
    "3) Keep doing this until you have skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730dd78",
   "metadata": {},
   "source": [
    "A __Transformer Architecture__ is generally considered to be any neural network that use the _attention mechanism_ as its primary learning layer. Similar to how a convolutional neural network uses convolutions as its primary learning layer.\n",
    "\n",
    "Like the name suggests, __the Vision Transformer (ViT) architecture was designed to adapt the original Transformer architecture to vision problem(s)__ (classification being the first and since many others have followed). \n",
    "\n",
    "The original Vision Transformer has been through several iterations over the past couple of years, however, we're going to focus on replicating the original, otherwise known as the \"vanilla Vision Transformer\". Because if you can recreate the original, you can adapt to the others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
