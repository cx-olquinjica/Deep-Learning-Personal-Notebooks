{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190b88cb",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf868b",
   "metadata": {},
   "source": [
    "The idea that we learn by interacting with our enviroment is probably the first to occcur to us when we think about the nature of learning. \n",
    "\n",
    "Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence. \n",
    "\n",
    "\n",
    "__Reinforcement Learning:__ is learning what to do- how to map situations to actions-so as to maximize a numerical reward signal. \n",
    "\n",
    "So one can think of Reinforcement learning as a mapping problem where we want to learn the mapping from situations to actions in other to maximize a numerical reward signal. This very definition gives us hints on the most importants componets of the field. \n",
    "\n",
    "In RL the learner is not told which actions to take, but instead, but instead must discover which actions yield the most reward by trying them.  In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. \n",
    "\n",
    "\n",
    "These are the two most important distiguishing features of reinforcement learning: \n",
    "- trial and error\n",
    "- delayed reward\n",
    "\n",
    "__Note__: RL, like many topics whose names end with \"ing\", such as machine learning and mountaineering, is simultaneously a problem, a class of solution methods that work well on the problem, and the field that studies this problem and its solution methods. \n",
    "\n",
    "The problem of RL is formalized using the Markov Decision Processes: the basic idea is simply to capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841e07d",
   "metadata": {},
   "source": [
    "__A learning agent must be able to sense the state of its environment to some extenxt and must be able to take actions that affect the state of its environment. The agent also must have a goal or goals relating to the state of the environment.__ _Markov decision processses_ are intended to include just these three aspects _sensation, action, and goal_ in their simplest possible forms without trivializing any of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ce1b0",
   "metadata": {},
   "source": [
    "## How is RL different from other Machine Learning Paradigms?\n",
    "\n",
    "a) __Supervised Learning:__ SL is learning from a training set of labeled examplse provided by a knowledgable external supervisor. Each example is a description of a situation together with a specification-the label of the correct action the system should take to that situation, which is oftern to identify a category to which the situation belongs. _The objective_ of this kind of learning is for the system to extrapolate, or generalize, its responses so that it acts correctly in situation not present in the training set. \n",
    "\n",
    "This is an important kind of learning, but alone it is not adequate for learning from interaction. In interactive problems it is often impractical to obtain examples of desired behavior that are both correct and representative of all the situations in which the agent has to act. In uncharted territory—where one would expect learning to be most beneficial—an agent must be able to learn from its own experience. \n",
    "\n",
    "In other words there is not __IID__ assumptions!\n",
    "\n",
    "\n",
    "b) __Unsupervised Learning__: it is typically about finding structure hidden in the collections of unlabeled data. The terms supervised learning and unsupervised learning would seem to exhaustively classify machine learning paradigms, but they do not. __Although one might be tempted to think of reinforcement learning as a kind of unsupervised learning because it does not rely on examples of correct behavior, reinforcement learning is trying to maximize a reward signal instead of trying to find hidden structure.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052ee4d",
   "metadata": {},
   "source": [
    "## Challenges: \n",
    "\n",
    "One of the challenges that arise in reinforcement learning, and not in other kinds of learning, is the trade-o↵ between exploration and exploitation. To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be e↵ective in producing reward. But to discover such actions, it has to try actions that it has not selected before. The agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future. The dilemma is that neither exploration nor exploitation can be pursued exclusively without failing at the task. The agent must try a variety of actions and progressively favor those that appear to be best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfccd97",
   "metadata": {},
   "source": [
    "## Elements of Reinforcement Learning\n",
    "\n",
    "- Agent\n",
    "- Environment \n",
    "Beyond these two there are four main subelements: \n",
    "\n",
    "- a policy\n",
    "- a reward signal \n",
    "- value function\n",
    "- (optionally) model\n",
    "\n",
    "__Policy__: A policy defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy __is a mapping from perceived states of the environment to actions to be taken when in those states__.\n",
    "\n",
    "It corresponds to what in psychology would be called a set of stimulus–response rules or associations. \n",
    "\n",
    "In some cases the policy may be a simple function or lookup table, whereas in others it may involve extensive computation such as a search process. The policy is the core of a reinforcement learning agent in the sense that it alone is sufficient to determine behavior. In general, policies may be stochastic, specifying probabilities for each action.\n",
    "\n",
    "__Reward__: defines the goal of a RL problem. On each time step, the environment sends to the reinforcement learning agent a single number called the reward. The agent’s sole objective is to maximize the total reward it receives over the long run. The reward signal thus defines what are the good and bad events for the agent. __In a biological system, we might think of rewards as analogous to the experiences of pleasure or pain.__\n",
    "\n",
    "The reward signal is the primary basis for altering the policy; if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future. In general, reward signals may be stochastic functions of the state of the environment and the actions taken.\n",
    "\n",
    "__Value Function__: Whereas the reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. __Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states.__\n",
    "\n",
    "For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards. Or the reverse could be true. To make a human analogy, rewards are somewhat like pleasure (if high) and pain (if low), whereas values correspond to a more refined and farsighted judgment of how pleased or displeased we are that our environment is in a particular state.\n",
    "\n",
    "__Please note:__ Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward. _Nevertheless, it is values with which we are most concerned when making and evaluating decisions. Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run._ Unfortunately, it is much harder to determine values than it is to determine rewards. Rewards are basically given directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime. __In fact, the most important component of almost all reinforcement learning algorithms we consider is a method for efficiently estimating values. The central role of value estimation is arguably the most important thing that has been learnend about reinforcement learning over the last six decades__. \n",
    "\n",
    "\n",
    "__Mode__: This is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave.\n",
    "\n",
    "For example, given a state and action, the model might predict the resultant next state and next reward. Models are used for planning, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced. Methods for solving reinforcement learning problems that use models and planning are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and-error learners—viewed as almost the opposite of planning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8661e7",
   "metadata": {},
   "source": [
    "## Limitations and Scope: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
