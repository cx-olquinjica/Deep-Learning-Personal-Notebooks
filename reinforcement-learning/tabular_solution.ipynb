{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cb9b5f",
   "metadata": {},
   "source": [
    "# Tabular Solution Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ba00b",
   "metadata": {},
   "source": [
    "__Tabular Solution Methods__: are those in which the state and action spaces are small enough for the approximate value functions to be represented as arrays, or tables.\n",
    "\n",
    "One of the characteristics of these methods is that they can often find exactly the optimal value function and the optimal policy. In contrast methods that find approxmate solutions can be applied effectively to much larger problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421674a9",
   "metadata": {},
   "source": [
    "# Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b65ebe",
   "metadata": {},
   "source": [
    "One of the most distinguising features of reinforcement learning from other type os learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. \n",
    "\n",
    "### A k-armed Bandit Problem\n",
    "\n",
    "Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n",
    "\n",
    "Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the conflict between exploration and exploitation. \n",
    "\n",
    "__In any specific case, wheter it is better to explore or exploit depends in a complex way on the precise values of the estimateas, uncertainties, and the number of remaining steps.__ There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formations of the k-amred bandit and related problems. However, most of these methods make strong assumptions about stationarity and prior knowledge that are either violated or impossible to verify in applications and in the full reinforcement learning problem.\n",
    "\n",
    "### Action-value Methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
