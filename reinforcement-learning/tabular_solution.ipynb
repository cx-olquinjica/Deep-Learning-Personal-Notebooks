{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cb9b5f",
   "metadata": {},
   "source": [
    "# Tabular Solution Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ba00b",
   "metadata": {},
   "source": [
    "__Tabular Solution Methods__: are those in which the state and action spaces are small enough for the approximate value functions to be represented as arrays, or tables.\n",
    "\n",
    "One of the characteristics of these methods is that they can often find exactly the optimal value function and the optimal policy. In contrast methods that find approxmate solutions can be applied effectively to much larger problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421674a9",
   "metadata": {},
   "source": [
    "# Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b65ebe",
   "metadata": {},
   "source": [
    "One of the most distinguising features of reinforcement learning from other type os learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. \n",
    "\n",
    "### A k-armed Bandit Problem\n",
    "\n",
    "Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n",
    "\n",
    "Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the conflict between exploration and exploitation. \n",
    "\n",
    "__In any specific case, wheter it is better to explore or exploit depends in a complex way on the precise values of the estimateas, uncertainties, and the number of remaining steps.__ There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formations of the k-amred bandit and related problems. However, most of these methods make strong assumptions about stationarity and prior knowledge that are either violated or impossible to verify in applications and in the full reinforcement learning problem.\n",
    "\n",
    "### About Value-Functions: \n",
    "\n",
    "Value functions answer the following questions: \n",
    "\n",
    "1. How good is it for the agent to be in a particular state?\n",
    "2. How good is it to perform a certain action in a given state?\n",
    "\n",
    "Please pay attention that __how good__ here indicates _future returns or rewards_ (i.e., the rewards the agent should expect from the future depends on what actions it will take). \n",
    "\n",
    "__Value-Function__: are defined with respect to particular ways of acting, called _policies_.\n",
    "__Policy__: mapping from states to probabilities of selecting each possible action. \n",
    "\n",
    "The value function of a state _s__ under policy _P_, denoted _vP_ is __the expected return when starting in _s_ and following _P_ thereafter__. \n",
    "\n",
    "There are two kind of value functions: \n",
    "- Action-value function\n",
    "- State-value function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
