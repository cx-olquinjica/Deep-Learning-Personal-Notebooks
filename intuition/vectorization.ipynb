{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bfea20",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc69aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5162d",
   "metadata": {},
   "source": [
    "## Why is Vectorization important? \n",
    "\n",
    "Because ML models are hungry for data, and these data are mainly represented by matrices, and performing matrix operations for large matrices is computational costly, not mentioning that you would need to write down lots of \"for loops\" to deal with every single possible case. Using vectorization you can perform normal matrix operations in a single line of code, and the speed is much faster than the _for loop version_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c7cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_size = 1000000\n",
    "x1_numpy = np.random.rand(1,array_size)\n",
    "x2_numpy = np.random.rand(1,array_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea796462",
   "metadata": {},
   "source": [
    "### For Loop Version: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f67ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product = 250121.23293094424\n",
      "Computation time = 383.8790000000001ms\n"
     ]
    }
   ],
   "source": [
    "tic = time.process_time()\n",
    "dot_product = 0\n",
    "for i in range(x1_numpy.shape[1]):\n",
    "    dot_product +=x1_numpy[0,i]*x2_numpy[0,i]\n",
    "toc = time.process_time()\n",
    "print(\"dot product = \" + str(dot_product))\n",
    "print (\"Computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c39e11",
   "metadata": {},
   "source": [
    "### Vectorized Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a3178e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product = [[250121.23293094]]\n",
      "Computation time = 5.665000000000031ms\n"
     ]
    }
   ],
   "source": [
    "tic = time.process_time()\n",
    "dot_product = np.dot(x1_numpy,x2_numpy.T)\n",
    "toc = time.process_time()\n",
    "print(\"dot product = \" + str(dot_product))\n",
    "print (\"Computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f58d00",
   "metadata": {},
   "source": [
    "Let's see what happens when we increase the size of the array to 1 Billion!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402717cd",
   "metadata": {},
   "source": [
    "### 1B For Loop Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ef8a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product = 249995743.31640688\n",
      "Computation time = 447705.865ms\n"
     ]
    }
   ],
   "source": [
    "A = np.random.rand(1, 1000000000)\n",
    "B = np.random.rand(1, 1000000000)\n",
    "tic = time.process_time()\n",
    "dot_product = 0\n",
    "for i in range(A.shape[1]):\n",
    "    dot_product +=A[0,i]*B[0,i]\n",
    "toc = time.process_time()\n",
    "print(\"dot product = \" + str(dot_product))\n",
    "print (\"Computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46274516",
   "metadata": {},
   "source": [
    "### Note: 447705.865ms is equivalent to 7.461764416667 minutes ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f543f96",
   "metadata": {},
   "source": [
    "### 1B Vectorized Version: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ffc5ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product = [[2.49995743e+08]]\n",
      "Computation time = 41866.602999999996ms\n"
     ]
    }
   ],
   "source": [
    "tic = time.process_time()\n",
    "dot_product = np.dot(A,B.T)\n",
    "toc = time.process_time()\n",
    "print(\"dot product = \" + str(dot_product))\n",
    "print (\"Computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179102a8",
   "metadata": {},
   "source": [
    "### Note: 41866.602999999996ms is equivalent to 0.6977767166666666 minute. Not even a minute! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f7829",
   "metadata": {},
   "source": [
    "## NOTE: I made sure to run this experiment on a mac computer and it took a very long time!! Not mentioning that it started making some noises lol. Hence not surprisingly, the difference between the two approaches becomes apparent as the array sizes increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad27fbf",
   "metadata": {},
   "source": [
    "I will leave this section here, so that I experiment with other deep learning framework, to understand how these libraries make our lives much easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
