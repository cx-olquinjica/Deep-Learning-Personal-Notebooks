{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac51fc28",
   "metadata": {},
   "source": [
    "# Parameter Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bb670",
   "metadata": {},
   "source": [
    "Once we have chosen an architecture and set our hyperparameters, we proceed to  the training loop, where our goal is to find parameters values that minimize our loss function. After training, we will need these parameters in order to make future predictions. Additionally, we will sometimes wish to extracct the parametres either to reuse them in some other context, to save our model to disk so that it may be executed in other software, or for examination in the hope of gaining scientific understanding. \n",
    "\n",
    "Although most of the time we are able to ignore the nitty-gritty details of how parameters are declared and manipulated, relying on deep learnng frameworkds to do the heavy lifting. However, when we move away from stacked architectures with standard layers, we will sometimes need to get into weeds of declaring and manipulating parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7640f204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7c626",
   "metadata": {},
   "source": [
    "## Parameters Access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839952e",
   "metadata": {},
   "source": [
    "When a model is defined vida the _Sequential_ class, we can first access any layer by indexing into the model as thoug it were a list. Each layer's parameters are conveniently located in its attribute. \n",
    "\n",
    "Inspecting the parameters of the second fully connected layers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97931d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.1552, -0.0060, -0.3226, -0.0647,  0.2093, -0.2392,  0.0093,  0.1994]])),\n",
       "             ('bias', tensor([0.1183]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f921dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b1e8",
   "metadata": {},
   "source": [
    "## Targeted Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64d6c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.parameter.Parameter, tensor([0.1183]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net[2].bias), net[2].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d200d3",
   "metadata": {},
   "source": [
    "Each parameters is represented as an instance of the parameters class. To do anything useful with the parameters, we first need to access the underlying numerical values. The code above extracts the bias from the second neural network layer, which retursn a parameter class instance, and further accesses that parameter's value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587504bb",
   "metadata": {},
   "source": [
    "## All Parameters at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d26251c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', torch.Size([8, 4])),\n",
       " ('0.bias', torch.Size([8])),\n",
       " ('2.weight', torch.Size([1, 8])),\n",
       " ('2.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a736a",
   "metadata": {},
   "source": [
    "## Tied Parameters\n",
    "\n",
    "Often, we want to share parameters across multiple layers. Let's see how to do this elegantly. In the following we allocate a fully connected layer and then use its parameters specifically to set those of another layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ffde77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f1002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
