{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597ca3e3",
   "metadata": {},
   "source": [
    "## Large-Scale Pretraining with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfddea79",
   "metadata": {},
   "source": [
    "The models I have encountered so far were trained on datasets with input-output examples from _scratch to perform specific tasks_. For example, a Transformer was trained with English-French pairs so that this model can translate input text into French. As a result, each model becomes a _specific expert_ that is sensitive to even slight shift in data distribution. For better generalized models, or even more competent _generalists_ that can perform multiple tasks with or without adaptation, _pretraining_ models on large data has been increasingly common. \n",
    "\n",
    "Given larger data for pretraining, the __Transformer architecture performs better with an increased model size and training compute, demonstrating super _scaling_ behaviour. Specifically, performance of Transformer-based language models scales as a power-law with the amount of model parameters, training tokens, and training compute.__ \n",
    "\n",
    "The scalability of Transformer is also evidenced by the significantly boosted performance from larger vision Transformers trained on larger data. \n",
    "\n",
    "Before compelling success of pretraining Transformers for multi-modal data, Transformers were extensively pretrained with a wealth of text. \n",
    "\n",
    "Primarily, Transformers can be used in three different modes: \n",
    "\n",
    "- Encoder-only\n",
    "- Encoder-decoder\n",
    "- Decoder-only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b717d",
   "metadata": {},
   "source": [
    "## Encoder-Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57c36d",
   "metadata": {},
   "source": [
    "When only the Transformer encoder is used, a sequence of input tokens is converted into the same number of representations that can be futher projected into the output. (e.g., classification). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a67549",
   "metadata": {},
   "source": [
    "## Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67389873",
   "metadata": {},
   "source": [
    "Since a Transformer encoder converts a sequence of input tokens into the same number of output representations, the encoder-only mode cannot generate a sequence of arbitrary length like in machine translation. As originally proposed for machine translation, the Transformer architecture can be outfitted with a decoder that autoregressively predicts the target sequence of arbitrary length, token by token, conditional on both encoder output and decoder output: (i) for conditioning on encoder output, encoder-decoder cross-attention (multi-head attention of decoder) allows target tokens to attend to all input tokens; (ii) conditioning on decoder output is achieved by a so-called causal attention (this name is common in the literature but is misleading as it has little connection to the proper study of causality) pattern (masked multi-head attention of decoder), where any target token can only attend to past and present tokens in the target sequence.\n",
    "\n",
    "To pretrain encoder-decoder Transformers beyond human-labeled machine translation data, BART (Lewis et al., 2019) and T5 (Raffel et al., 2020) are two concurrently proposed encoder-decoder Transformers pretrained on large-scale text corpora. Both attempt to reconstruct original text in their pretraining objectives, while the former emphasizes noising input (e.g., masking, deletion, permutation, and rotation) and the latter highlights multitask unification with comprehensive ablation studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9023cb",
   "metadata": {},
   "source": [
    "## Decoder-Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a709a",
   "metadata": {},
   "source": [
    "Alternatively, decoder-only Transformers remove the entire encoder and the decoder sublayer with the encoder-decoder cross-attention from the original encoder-decoder architecture. \n",
    "\n",
    "Nowadays, decoder-only Transformers have been the de facto architecture in large-scale language modeling, which leverages the worldâ€™s abundant unlabeled text corpora via self-supervised learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
