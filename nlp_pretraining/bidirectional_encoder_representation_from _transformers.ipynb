{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc0761a",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representation from Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835a406",
   "metadata": {},
   "source": [
    "## Input Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06623c7",
   "metadata": {},
   "source": [
    "In natural language processing, some tasks (e.g., sentiment analysis) take single text as input, while in some other tasks (e.g., natural langauge inference), the input is a pair of text sequences. The __BERT__ input sequence unambigously represents both single text and text pairs. \n",
    "\n",
    "### Single Text: \n",
    "\n",
    "BERT input sequence is the concatenation of the special classification token <cis>, tokens of a text sequence, and the special separation token <sep>.\n",
    "\n",
    "### Text Pairs: \n",
    "    \n",
    "BERT input sequence is the concatenation of <cls>, tokens of the first text sequence, <sep>, tokens of the second text sequence, and <sep>. We will consistently distinguish the terminology “BERT input sequence” from other types of “sequences”. For instance, one BERT input sequence may include either one text sequence or two text sequences.\n",
    "    \n",
    "The folowing code takes eitheer one sentence or two sentences as input, then returns tokens of the BERT input sequence and their corresponding segment IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333db75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5d4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0 and 1 are marking segment A and B, respectively\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af025d",
   "metadata": {},
   "source": [
    "BERT chooses the Transformer encoder as its bidirectional architecture. Common in the Transformer encoder, positional embeddings are added at every position of the BERT input sequence. However, different from the original Transformer encoder, BERT uses learnable positional embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ffd917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_blks, dropout, max_len=1000, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", d2l.TransformerEncoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # In BERT, positional embeddings are learnable, thus we create a\n",
    "        # parameter of positional embeddings that are long enough\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Shape of `X` remains unchanged in the following code snippet:\n",
    "        # (batch size, max sequence length, `num_hiddens`)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410d815",
   "metadata": {},
   "source": [
    "Suppose that the vocabulary size is 10000. To demostrate forwared inference of BERTEncoder, let's create an instance of it and intialize its parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efba2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "ffn_num_input, num_blks, dropout = 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                      num_blks, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22f7ed",
   "metadata": {},
   "source": [
    "We define tokens to be 2 BERT input sequences of length 8, where each token is an index of the vocabulary. The forward inference of BERTEncoder with the input tokens returns the encoded result where each token is represented by a vector whose length is predefined by the hyperparameter num_hiddens. This hyperparameter is usually referred to as the hidden size (number of hidden units) of the Transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e35090da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.randint(0, vocab_size, (2, 8))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
