{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7cc3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l # they have a torch library inside d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11209a5c",
   "metadata": {},
   "source": [
    "## Understanding d2l Libray:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268f6fa",
   "metadata": {},
   "source": [
    "## Utilities:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c4a68",
   "metadata": {},
   "source": [
    "- def add_to_class: It allows us to register functions as methods in a class after the class has been created. i.e, add_to_class(class), and then followed by the class\n",
    "- class HyperParameters: it saves all arguments in a class _init_ method as class attributes.\n",
    "- class ProgressBoard: it allows to plot experiments progress interactively while it is goind on. NOTE: it is a subclass of the HyperParameters class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e009c659",
   "metadata": {},
   "source": [
    "## Module:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a4dba",
   "metadata": {},
   "source": [
    "The module class is the base class of all models we will implement. At a minimum we need to define three methods:\n",
    "\n",
    "- The _init_ method: stores learnable parameters.\n",
    "- The training_step: accepts a data batch to return the loss value.\n",
    "- The Configure_optimizers: returns the optmization method, or a list of them, that is used to update the learnable parameters. \n",
    "- (optionally) we can define validation_step to report the evaluation measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da2665",
   "metadata": {},
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "552e3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module, d2l.HyperParameters):  #@save\n",
    "    \"\"\"The base class of models.\"\"\"\n",
    "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.board = ProgressBoard()\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X):\n",
    "        assert hasattr(self, 'net'), 'Neural network is defined'\n",
    "        return self.net(X)\n",
    "\n",
    "    def plot(self, key, value, train):\n",
    "        \"\"\"Plot a point in animation.\"\"\"\n",
    "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
    "        self.board.xlabel = 'epoch'\n",
    "        if train:\n",
    "            x = self.trainer.train_batch_idx / \\\n",
    "                self.trainer.num_train_batches\n",
    "            n = self.trainer.num_train_batches / \\\n",
    "                self.plot_train_per_epoch\n",
    "        else:\n",
    "            x = self.trainer.epoch + 1\n",
    "            n = self.trainer.num_val_batches / \\\n",
    "                self.plot_valid_per_epoch\n",
    "        self.board.draw(x, value.to(d2l.cpu()).detach().numpy(),\n",
    "                        ('train_' if train else 'val_') + key,\n",
    "                        every_n=int(n))\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def apply_init(self, inputs, init=None):\n",
    "        self.forward(*inputs)\n",
    "        if init is not None:\n",
    "            self.net.apply(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e0dfc",
   "metadata": {},
   "source": [
    "NOTE: You may notice that Module is a subclass of nn.Module, the base class of neural networks in PyTorch. It provides convenient features to handle neural networks. For example, if we define a forward method, such as forward(self, X), then for an instance a we can invoke this method by a(X). This works since it calls the forward method in the built-in __call__ method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5281fb",
   "metadata": {},
   "source": [
    "## Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9564cc4",
   "metadata": {},
   "source": [
    "The __DataModule__ class is the base class for data. Quite frequently the _init_ method is used to prepare the data. This includes downloading and preprocessing if needed. Important methods: \n",
    "\n",
    "- train_dataloader: returns the data loader for the training dataset. (it is used in the training_step in __Module__)\n",
    "- val_dataloader: returns the validation dataset loader. (used for the validatio_step in __Module_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5051a",
   "metadata": {},
   "source": [
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7816278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(d2l.HyperParameters):  #@save\n",
    "    \"\"\"The base class of data.\"\"\"\n",
    "    def __init__(self, root='../data', num_workers=4):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46968432",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93709036",
   "metadata": {},
   "source": [
    "The __Trainer__ class trains the learnable parameters in the __Module__ class with data specified in DataModule. The key method is:\n",
    "- __fit__, which accepts two arguments:\n",
    "    - _model_ an instance of Module.\n",
    "    - _data_ an instance of DataModule\n",
    "    \n",
    "It then iterates over the entire dataset max_epochs times to train the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639aa4bc",
   "metadata": {},
   "source": [
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c594b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(d2l.HyperParameters):  #@save\n",
    "    \"\"\"The base class for training models with data.\"\"\"\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        self.save_hyperparameters()\n",
    "        assert num_gpus == 0, 'No GPU support yet'\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        self.num_train_batches = len(self.train_dataloader)\n",
    "        self.num_val_batches = (len(self.val_dataloader)\n",
    "                                if self.val_dataloader is not None else 0)\n",
    "\n",
    "    def prepare_model(self, model):\n",
    "        model.trainer = self\n",
    "        model.board.xlim = [0, self.max_epochs]\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, model, data):\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = model.configure_optimizers()\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "\n",
    "    def fit_epoch(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934001d3",
   "metadata": {},
   "source": [
    "# FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4d04b",
   "metadata": {},
   "source": [
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf88d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(d2l.DataModule):  #@save\n",
    "    \"\"\"The Fashion-MNIST dataset.\"\"\"\n",
    "    def __init__(self, batch_size=64, resize=(28, 28)):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        trans = transforms.Compose([transforms.Resize(resize),\n",
    "                                    transforms.ToTensor()])\n",
    "        self.train = torchvision.datasets.FashionMNIST(\n",
    "            root=self.root, train=True, transform=trans, download=True)\n",
    "        self.val = torchvision.datasets.FashionMNIST(\n",
    "            root=self.root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b782ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(FashionMNIST)  #@save\n",
    "def text_labels(self, indices):\n",
    "    \"\"\"Return text labels.\"\"\"\n",
    "    labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "              'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [labels[int(i)] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ea0cb",
   "metadata": {},
   "source": [
    "# The Base Classification Model\n",
    "\n",
    "This section provides a base class for classification models to simplify future code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c6496f",
   "metadata": {},
   "source": [
    "We define the Classifier class below. In the validation_step we report both the loss value and the classification accuracy on a validation batch. We draw an update for every num_val_batches batches. This has the benefit of generating the averaged loss and accuracy on the whole validation data. These average numbers are not exactly correct if the last batch contains fewer examples, but we ignore this minor difference to keep the code simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0547598",
   "metadata": {},
   "source": [
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2dc546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(d2l.Module):  #@save\n",
    "    \"\"\"The base class of classification models.\"\"\"\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "    \n",
    "  \n",
    "    def accuracy(self, Y_hat, Y, averaged=True):\n",
    "        \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "        Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
    "        preds = Y_hat.argmax(axis=1).type(Y.dtype)\n",
    "        compare = (preds == Y.reshape(-1)).type(torch.float32)\n",
    "        return compare.mean() if averaged else compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032e31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
