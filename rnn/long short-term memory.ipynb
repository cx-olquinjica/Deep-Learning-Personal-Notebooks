{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e026f37d",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aa2e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d74d3a",
   "metadata": {},
   "source": [
    "The term __long short-term memory__ comes from the following intuition: Simple recurrent neural networks have _long-term memory_ in the form of weights. The weights change slowly during training, enconding general knowledge about the data. They also have _short-term memory_ in the form of ephemeral activations, which pass from each node to successive nodes. \n",
    "\n",
    "LSTMs resemble standard recurrent neural networks but here each ordinary recurrent node is replaced by a _memory cell_. Each memory cell contains an _internal state_, i.e., a node with a self-connected recurrent edge of fixed weight 1, ensuring that the gradient can pass across many time steps without vanishing or exploding.\n",
    "\n",
    "The LSTM model introduces an intermediate type of storage via the memory cell. A memory cell is a composite unit, built from simpler nodes in a specific cnnectivity patter, with the novel inclusion of multiplicative nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242dae3",
   "metadata": {},
   "source": [
    "## Gated Memory Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5723a4b6",
   "metadata": {},
   "source": [
    "Each memory cell is equipped with an internal state and a number of multiplicative gates that determine whether: \n",
    "\n",
    "1) a given input should impact the internal state (the input gate), \n",
    "2) the internal state should be flushed to 0 (the forget gate), and\n",
    "3) the internal state of a given neuron should be allowed to impact the cell's output (the output gate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9121b",
   "metadata": {},
   "source": [
    "### a) Gated Hidden State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2c863",
   "metadata": {},
   "source": [
    "The key distinction between vanilla RNNs and LSTMs is that the latter support gating of the hidden state. This means that we have dedicated mechanisms for when a hidden stae should be _updated_ and also when it should be _reset_. These mechanisms are learned and they address the concerns listed above. For instance, if the first token is of great importance we will learn not to update the hidden after the first observation. Likewise, we will learn to skip irrelevant temporary observations. Last, we will learn to reset the latent state whenever needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee7c84",
   "metadata": {},
   "source": [
    "### b) Input Gate, Forget Gate, and Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db31cd",
   "metadata": {},
   "source": [
    "The data feeding into the LSTM gates are __the input at the current time step__ and __the hidden state of the previous time step__. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
