{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2b19d7",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1eb40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544292fd",
   "metadata": {},
   "source": [
    "## Learning Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa535e",
   "metadata": {},
   "source": [
    "* Markov Models and n-grams\n",
    "* Word Frequency\n",
    "* Laplace Smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77bcc9",
   "metadata": {},
   "source": [
    "### How to measure the language model quality?\n",
    "\n",
    "One way is to check _how surprising the text is__. __A good language model is able to predict tokens with high accuracy.__\n",
    "\n",
    "Using cross-entropy? Perplexity? \n",
    "\n",
    "We will design language models using neural networks and use perplexity to evaluate how good the model is at predicting the next token given the current set of tokens in the text sequences. \n",
    "\n",
    "To train language models, we can randomly sample pairs of input sequences and target minibatches. After training, we will use perplexity to measure the language model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd90b98",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28ddcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.TimeMachine)  #@save\n",
    "def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\n",
    "    super(d2l.TimeMachine, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    corpus, self.vocab = self.build(self._download())\n",
    "    array = torch.tensor([corpus[i:i+num_steps+1]\n",
    "                        for i in range(len(corpus)-num_steps)])\n",
    "    self.X, self.Y = array[:,:-1], array[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cee2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.TimeMachine)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    idx = slice(0, self.num_train) if train else slice(\n",
    "        self.num_train, self.num_train + self.num_val)\n",
    "    return self.get_tensorloader([self.X, self.Y], train, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c692574d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 7, 19, 16, 14,  0, 21,  9,  6,  0, 17],\n",
      "        [21,  9,  2, 21,  0,  7, 16, 13, 13, 16]]) \n",
      "Y: tensor([[19, 16, 14,  0, 21,  9,  6,  0, 17, 19],\n",
      "        [ 9,  2, 21,  0,  7, 16, 13, 13, 16, 24]])\n"
     ]
    }
   ],
   "source": [
    "data = d2l.TimeMachine(batch_size=2, num_steps=10)\n",
    "for X, Y in data.train_dataloader():\n",
    "    print('X:', X, '\\nY:', Y)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
