{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7e792d",
   "metadata": {},
   "source": [
    "# The Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c35052",
   "metadata": {},
   "source": [
    "In general seq2seq problems like machine translation, _inputs and outputs are of varying lengths that are unaligned_.\n",
    "\n",
    "The standard approach to handling this sort of data is to design an encoder-decoder architecture consisting of two major components: \n",
    "\n",
    "- __An encoder:__ that takes a variable-length sequence as input, \n",
    "- __And a decoder:__ that acts as a _conditional language model_, taking in the encoded input and the leftwards context of the target sequence and predicting the subsequent token in the target sequence.\n",
    "\n",
    "Let’s take machine translation from English to French as an example. Given an input sequence in English: “They”, “are”, “watching”, “.”, this encoder-decoder architecture:\n",
    "\n",
    "* first encodes the variable-length input into a state, \n",
    "* then decodes the state to generate the translated sequence, token by token, as output: “Ils”, “regardent”, “.”. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9762f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b5acd",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "In the encoder interface, we just specify that the encoder takes variable-length sequences as input X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dfe9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  #@save\n",
    "    \"\"\"The base encoder interface for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Later there can be additional arguments (e.g., length excluding padding)\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca802a40",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "In the following decoder interface, we add an additional init_state method to convert the encoder output (enc_all_outputs) into the encoded state. Note that this step may require extra inputs, such as the valid length of the input. To generate a variable-length sequence token by token, every time the decoder may map an input (e.g., the generated token at the previous time step) and the encoded state into an output token at the current time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e0ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):  #@save\n",
    "    \"\"\"The base decoder interface for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Later there can be additional arguments (e.g., length excluding padding)\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45b97a",
   "metadata": {},
   "source": [
    "## Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0fada8",
   "metadata": {},
   "source": [
    "In the forward propagation, the output of the encoder is used to produce the encoded state, and this state will be further used by the decoder as one of its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea39944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(d2l.Classifier):  #@save\n",
    "    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_all_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "        # Return decoder output only\n",
    "        return self.decoder(dec_X, dec_state)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc57d42",
   "metadata": {},
   "source": [
    "Encoder-decoder architectures can handle inputs and outputs that both consist of variable-length sequences and thus are suitable for seq2seq problems such as machine translation. The encoder takes a variable-length sequence as input and transforms it into a state with a fixed shape. The decoder maps the encoded state of a fixed shape to a variable-length sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
