{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed13108",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Seq2Seq for Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d508f9a",
   "metadata": {},
   "source": [
    "Here we will implement an Encoder-Decoder architecture using a Recurrent Neural Network to the task of machine translation. The encoder RNN will take a variable-length sequence as input and transform it into a fixed-shape hidden state. \n",
    "\n",
    "(__Side Note__): However, attention mechanisms allow us to access encoded inputs without having to compress the entire input into a single fixed-length representation.\n",
    "\n",
    "Then to generate the output sequence, one token at a time, the decoder model, consisting of a separate RNN, will predict each successive target token given both the input sequence and the preceding tokens in the output. During training, the decoder will typically be conditioned upon the preceding tokens in the official “ground-truth” label. However, at test time, we will want to condition each output of the decoder on the tokens already predicted. Note that if we ignore the encoder, the decoder in a seq2seq architecture behaves just like a normal language model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
