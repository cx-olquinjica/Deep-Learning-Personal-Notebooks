{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e8b8f4",
   "metadata": {},
   "source": [
    "# Bidirectional Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9370c",
   "metadata": {},
   "source": [
    "So far, our working example of a sequence learning task has been language modeling, where we aim to predict the next token given all previous tokens in a sequence. In this scenario, we wish only to condition upon the leftward context, and thus the unidirectional chaining of a standard RNN seems appropriate. __However, there are many other sequence learning tasks contexts where it is perfectly fine to condition the prediction at every time step on both the leftward and the rightward context.__ Consider, for example, part of speech detection. Why shouldnâ€™t we take the context in both directions into account when assessing the part of speech associated with a given word?\n",
    "\n",
    "Fortunately, a simple technique transforms any unidirectional RNN into a bidirectional RNN [Schuster and Paliwal, 1997](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=650093). __We simply implement two unidirectional RNN layers chained together in opposite directions and acting on the same input.__\n",
    "\n",
    "\n",
    "(Mathematical Intuition. Look at the equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88be35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40775d6",
   "metadata": {},
   "source": [
    "## Implementation from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a66b2e",
   "metadata": {},
   "source": [
    "To implement a bidirectional RNN from scratch, we can include two unidirectional RNNScratch instances with separate learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c2c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNNScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.num_hiddens *= 2  # The output dimension will be doubled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbccc5d6",
   "metadata": {},
   "source": [
    "To implement a bidirectional RNN from scratch, we can include two unidirectional RNNScratch instances with separate learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f271208",
   "metadata": {},
   "source": [
    "__Forward Method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c3ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(BiRNNScratch)\n",
    "def forward(self, inputs, Hs=None):\n",
    "    f_H, b_H = Hs if Hs is not None else (None, None)\n",
    "    f_outputs, f_H = self.f_rnn(inputs, f_H)\n",
    "    b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n",
    "    outputs = [torch.cat((f, b), -1) for f, b in zip(\n",
    "        f_outputs, reversed(b_outputs))]\n",
    "    return outputs, (f_H, b_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926af55c",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096ef2b",
   "metadata": {},
   "source": [
    "Using the high-level APIs, we can implement bidirectional RNNs more concisely. Here we take a GRU model as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3d074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(d2l.RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)\n",
    "        self.num_hiddens *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12baf9cf",
   "metadata": {},
   "source": [
    "__In bidirectional RNNs, the hidden state for each time step is simultaneously determined by the data prior to and after the current time step. Bidirectional RNNs are mostly useful for sequence encoding and the estimation of observations given bidirectional context. Bidirectional RNNs are very costly to train due to long gradient chains.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
