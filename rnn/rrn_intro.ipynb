{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc133cef",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db44b3b",
   "metadata": {},
   "source": [
    "Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that models product outputs consisting of sequences. In other domains, such as time series pediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. These demands often arise simualteneously: tasks such as translating passages of text from one natural language to another, enganging in dialogue, or controlling a robot, demand that models both ingest and output sequentially-structured data. \n",
    "\n",
    "__Recurrent neural networks (RNNs)__ are deep learning models that capture the dynamics of sequences via recurrent connections, which can be thought of as cycles in the network of nodes. This might seem counterintuitive at first. _After all, it is the feedforward nature of neural networks that makes the order of computation unambiguous_. However, recurrent edges are defined in a precise way that ensures that no such ambiguity can arise. Recurrent neural networks are unrolled across time steps (or sequence steps), with the same underlying parameters applied at each step. While the standard connections are applied synchronously to propagate each layerâ€™s activations to the subsequent layer at the same time step, the recurrent connections are dynamic, passing information across adjacent time steps.\n",
    "\n",
    "Like neural networks more broadly, RNNs have a long discipline-spanning history, originating as models of the brain popularized by cognitive scientists and subsequently adopted as practical modeling tools employed by the machine learning community. As with deep learning more broadly, this book adopts the machine learning perspective, focusing on RNNs as practical tools which rose to popularity in the 2010s owing to breakthrough results on such diverse tasks as handwriting recognition (Graves et al., 2008), machine translation (Sutskever et al., 2014), and recognizing medical diagnoses (Lipton et al., 2016). We point the reader interested in more background material to a publicly available comprehensive review (Lipton et al., 2015). We also note that sequentiality is not unique to RNNs. For example, the CNNs that we already introduced can be adapted to handle data of varying length, e.g., images of varying resolution. Moreover, RNNs have recently ceded considerable market share to Transformer models, which will be covered in Section 11. However, RNNs rose to prominence as the default models for handling complex sequential structure in deep learning, and remain staple models for sequential modeling to this day. The stories of RNNs and of sequence modeling are inextricably linked, and this is as much a chapter about the ABCs of sequence modeling problems as it is a chapter about RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a8064",
   "metadata": {},
   "source": [
    "### Goal: \n",
    "__Predict sequentially structured targets based on sequentially structured inputs (e.g., machine translation or video captioning).__\n",
    "\n",
    "Such sequence-to-sequence tasks take two forms: \n",
    "\n",
    "1) __aligned__: where the input at each time step aligns with a corresponding target(e.g., part of speech tagging);\n",
    "2) __unaligned__: where the input and target do not necessarily exhibit a step-for-step correspondence (e.g., machine translation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56433aaa",
   "metadata": {},
   "source": [
    " ## Problem: \n",
    " __Unsupervised density modeling__ (also called sequence modeling). Here, given a collection of sequences, our goal is to estimate the probability mass function that tells us how likely we are to see any given sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8112b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch \n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f2f01",
   "metadata": {},
   "source": [
    "## Autoregressive Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
