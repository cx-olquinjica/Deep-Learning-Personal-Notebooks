{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc133cef",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db44b3b",
   "metadata": {},
   "source": [
    "Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that models product outputs consisting of sequences. In other domains, such as time series pediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. These demands often arise simualteneously: tasks such as translating passages of text from one natural language to another, enganging in dialogue, or controlling a robot, demand that models both ingest and output sequentially-structured data. \n",
    "\n",
    "__Recurrent neural networks (RNNs)__ are deep learning models that capture the dynamics of sequences via recurrent connections, which can be thought of as cycles in the network of nodes. This might seem counterintuitive at first. _After all, it is the feedforward nature of neural networks that makes the order of computation unambiguous_. However, recurrent edges are defined in a precise way that ensures that no such ambiguity can arise. Recurrent neural networks are unrolled across time steps (or sequence steps), with the same underlying parameters applied at each step. While the standard connections are applied synchronously to propagate each layerâ€™s activations to the subsequent layer at the same time step, the recurrent connections are dynamic, passing information across adjacent time steps.\n",
    "\n",
    "Like neural networks more broadly, RNNs have a long discipline-spanning history, originating as models of the brain popularized by cognitive scientists and subsequently adopted as practical modeling tools employed by the machine learning community. As with deep learning more broadly, this book adopts the machine learning perspective, focusing on RNNs as practical tools which rose to popularity in the 2010s owing to breakthrough results on such diverse tasks as handwriting recognition (Graves et al., 2008), machine translation (Sutskever et al., 2014), and recognizing medical diagnoses (Lipton et al., 2016). We point the reader interested in more background material to a publicly available comprehensive review (Lipton et al., 2015). We also note that sequentiality is not unique to RNNs. For example, the CNNs that we already introduced can be adapted to handle data of varying length, e.g., images of varying resolution. Moreover, RNNs have recently ceded considerable market share to Transformer models, which will be covered in Section 11. However, RNNs rose to prominence as the default models for handling complex sequential structure in deep learning, and remain staple models for sequential modeling to this day. The stories of RNNs and of sequence modeling are inextricably linked, and this is as much a chapter about the ABCs of sequence modeling problems as it is a chapter about RNNs.\n",
    "\n",
    "Up untill now, we have focused on models whose inputs consisted of a single feature vector __x__. The main change of perspective when developing models capable of processing sequences is that we now focus on inputs that consist of _an ordered list of feature vectors_ __x1, ... , xt__ where each feature vecotr __xt__ indexed by a time step _t_ C Z lies in R. \n",
    "\n",
    "\n",
    "### Probability Distribution\n",
    "\n",
    "\n",
    "_Previously, when dealing with individual inputs, we assumed that they were sampled independently from the same underlying distribution ._ __While we still assume that entire sequences (e.g., entire documents or patient trajectories) are sampled independently, we cannot assume that the data arriving at each time step are independent of each other.__ For example, what words are likely to appear later in a document depends heavily on what words occurred earlier in the document. What medicine a patient is likely to receive on the 10th day of a hospital visit depends heavily on what transpired in the previous nine days.\n",
    "\n",
    "\n",
    "### Question: What are the implications of this statement?\n",
    "\n",
    "\n",
    "_This should come as no surprise._ __If we did not believe that the elements in a sequence were related, we would not have bothered to model them as a sequence in the first place.__ Consider the usefulness of the auto-fill features that are popular on search tools and modern email clients. They are useful precisely because it is often possible to predict (imperfectly, but better than random guessing) what likely continuations of a sequence might be, given some initial prefix. __For most sequence models, we do not require independence, or even stationarity, of our sequences. Instead, we require only that the sequences themselves are sampled from some fixed underlying distribution over entire sequences.__\n",
    "\n",
    "### Other implications: \n",
    "\n",
    "This flexible approach, allows for such phenomena as:\n",
    "\n",
    "1) documents looking significantly different at the beginning than at the end, or \n",
    "2) patient status evolving either towards recovery or towards death over the course of a hospital stay; and \n",
    "3) customer taste evolving in predictable ways over course of continued interaction with a recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a8064",
   "metadata": {},
   "source": [
    "### Goal: \n",
    "__Predict sequentially structured targets based on sequentially structured inputs (e.g., machine translation or video captioning).__\n",
    "\n",
    "Such sequence-to-sequence tasks take two forms: \n",
    "\n",
    "1) __aligned__: where the input at each time step aligns with a corresponding target(e.g., part of speech tagging);\n",
    "2) __unaligned__: where the input and target do not necessarily exhibit a step-for-step correspondence (e.g., machine translation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56433aaa",
   "metadata": {},
   "source": [
    " ## Problem: \n",
    " __Unsupervised density modeling__ (also called sequence modeling). Here, given a collection of sequences, our goal is to estimate the probability mass function that tells us how likely we are to see any given sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8112b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch \n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f2f01",
   "metadata": {},
   "source": [
    "## Autoregressive Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
