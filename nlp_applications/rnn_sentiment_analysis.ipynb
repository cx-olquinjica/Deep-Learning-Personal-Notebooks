{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f547956e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Using Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e4a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c6692a",
   "metadata": {},
   "source": [
    "## Representing Single Text with RNNs\n",
    "\n",
    "In text classifications tasks, such as sentiment analysis, a varying-length text sequence will be transformed into fixed-length categories. In the following __BiRNN__ class, while each token of a text sequence gets its individual pretrained GloVe representation via the embedding layer (self.embedding), the entire sequece is encoded by a bidirectional RNN (self.encoder). More concretely, the hidden states (at the last layer) of the bidirectional LSTM at both the initial and final time steps are concatenated as the representation of the text sequence. This single text representation is then transfomred into output categories by a fully connected layer (self.decoder) with two (\"positive\" and \"negative\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d4efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Set `bidirectional` to True to get a bidirectional RNN\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # The shape of `inputs` is (batch size, no. of time steps). Because\n",
    "        # LSTM requires its input's first dimension to be the temporal\n",
    "        # dimension, the input is transposed before obtaining token\n",
    "        # representations. The output shape is (no. of time steps, batch size,\n",
    "        # word vector dimension)\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        self.encoder.flatten_parameters()\n",
    "        # Returns hidden states of the last hidden layer at different time\n",
    "        # steps. The shape of `outputs` is (no. of time steps, batch size,\n",
    "        # 2 * no. of hidden units)\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        # Concatenate the hidden states at the initial and final time steps as\n",
    "        # the input of the fully connected layer. Its shape is (batch size,\n",
    "        # 4 * no. of hidden units)\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cc523",
   "metadata": {},
   "source": [
    "Let's construct a bidirectional RNN with two hidden layers to represent single text for sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c21217",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, devices = 100, 100, 2, d2l.try_all_gpus()\n",
    "net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)\n",
    "\n",
    "def init_weights(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "    if type(module) == nn.LSTM:\n",
    "        for param in module._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(module._parameters[param])\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0aff0",
   "metadata": {},
   "source": [
    "## Loading Pretrained Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e901f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/glove.6B.100d.zip from http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.100d.zip...\n"
     ]
    }
   ],
   "source": [
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868757d",
   "metadata": {},
   "source": [
    "Print the shape of the vectors for all the tokens in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8907a41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49346, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3d724",
   "metadata": {},
   "source": [
    "We use these pretrained word vectors to represent tokens in the reviews and will not updatae these vectors during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6e139",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631cd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
