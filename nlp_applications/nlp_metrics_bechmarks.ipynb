{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261b0427",
   "metadata": {},
   "source": [
    "# Resources and Benchmarks for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c077a72",
   "metadata": {},
   "source": [
    "The need to understand the metrics and resources in NLP arrive from the necessity to, for example evaluate two different models which one is better suited to the task? TL;DR _benchmarking_.\n",
    "\n",
    "This notebook will explore some of the most commonly used benchmarking datasets and pre-training resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9fe70d",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "One concept that naturally emerged from the idea of generating datasets and benchmarking, is the concept of a _leaderboard_ in which different models are compared with each other. Depending on the task, the models are evaluated with different metrics.\n",
    "\n",
    "__Exact Match (EM):__ The percentage of predictions that match any one of the answers exactly. \n",
    "\n",
    "__(Macro-averaged) F1 score (F1):__ Each answer and prediction is tokenized into words. For every answer to a given question, the overlap between the prediction and each answer is calculated and the maximum F1 is chosen. This score is then averaged over all the questions. \n",
    "\n",
    "However, F1 score is closely related to two other terms common in classification models, they are all ways to measure the accuracy of a model.\n",
    "\n",
    "Precision, Recall, and F1 use positives and negatives to meausure a model accuracy when making predictions. The three of them meausure different things. Let's explain their differences, using an analogy for a model that predicts apples and bananas: \n",
    "\n",
    "    - Class A = Apple\n",
    "    - Class B = Bananas\n",
    "If your model avoids a lot of mistakes in predicting apples and bananas, then your model has a high precision.\n",
    "\n",
    "If your model avoids a lot of mistakes in predicting apples __as__ bananas, then your model has a high recall.\n",
    "\n",
    "The goal is to aim high for both _recall and precision_. \n",
    "\n",
    "But what if your model is so good at predicting one class but sucks at predicting the other? tl;dr it would be misleading to simply look at precion and recall in isolation, this is where __F1__ comes in.\n",
    "\n",
    "F1 takes into account both precision and recall. A balance of two is where F1 scores on. If your model does a good job at predicting apple and bananas then you will have a high F1 score.\n",
    "\n",
    "\n",
    "__Perplexity:__ Perplexity is a measurement of how well a probability model predicts a sample. \n",
    "\n",
    "A low perplexity indicates the probability distribution is good at predicting the sample. In NLP, perplexity is a way of evaluating language models. \n",
    "\n",
    "__BLEU (Bilingual Evaluation Understudy):__ is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Scores are calculated for individual translated segments-generally sentences-by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. However, intelligibility or grammatical correctness are not taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92149dfd",
   "metadata": {},
   "source": [
    "## Benchmark Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f1d29",
   "metadata": {},
   "source": [
    "### SQuAD\n",
    "\n",
    "The first Version of the Stanford Question Answering Dataset was released in 2016. The dataset was created with the aim of advancing the field of reading comprehension. Reading text and answering questions about it is a demanding task for machines and requires large data sets of high quality. Most of the datasets before the release of the first version of SQuAD were either of high quality or of large size, but not both.\n",
    "\n",
    "With the help of crowdworkers, 107.785 question-answer pairs were created for 536 Wikipedia articles. For each question, the answer is a segment of text, or span, from the corresponding reading passage. Pairs were collected in a two-step process. In the first step the crowdworkers were asked to generate five questions and their answers per paragraph.\n",
    "\n",
    "In the second step, each crowdworker was shown only the questions along with the paragraphs of the corresponding article and was asked to choose the shortest span in the paragraph that answered the question. As a result of this process, questions in the dev-set multiple answers.\n",
    "\n",
    "The goal of this procedure was to get a more robust evaluation and to obtain an indicator of human performance on SQuAD.\n",
    "\n",
    "One shortcoming of reading comprehension systems is that they tend to make unreliable guesses on questions to which no correct answer is possible. With this in mind, the second version of SQuAD was released in 2018. In addition to the approximately 100.000 questions from the first version, 53.775 new, unanswerable questions on the same paragraphs are contained in this dataset.\n",
    "\n",
    "The accuracy of models trained on SQuAD is evaluated using two different metrics, exact match and (Macro-averaged) F1 score, both ignoring punctuation and articles.\n",
    "\n",
    "To evaluate human performance, the second answer to each question is treated as the human prediction. (Rajpurkar et al. 2016; Rajpurkar, Jia, and Liang 2018)\n",
    "\n",
    "Humans achieve an EM score of 86.831 and a F1 score of 89.452.\n",
    "\n",
    "Currently, the best performing model achieves an EM score of 90.386 and a F1 score of 92.777.\n",
    "\n",
    "Examples of SQuAD and the leaderboard and can be viewed here:\n",
    "\n",
    "https://rajpurkar.github.io/SQuAD-explorer/\n",
    "\n",
    "### CoQA\n",
    "\n",
    "CoQA is a dataset for building Conversational Question Answering systems. Humans are capable of gathering information through conversations that include several interrelated questions and answers. The aim of CoQA is to enable machines to answers conversational questions.\n",
    "\n",
    "The data set is made up of 127k Q/A pairs, covering seven different domains such as Childrenâ€™s Stories or Reddit. Five of these domains are used for in-domain evaluation, meaning models have already seen questions from these domains, and two are used for out-of-domain evaluation., meaning models have not seen any questions from these domains. To create the Q/A pairs, two people received a text passage, with one person asking the other person questions about the text and the other person answering. Using multiple annotators has a few advantages:\n",
    "\n",
    "A natural flow of conversation is created.\n",
    "If one person gives an incorrect answer or a vague questions is asked, the other person can raise a flag. Thus bad annotators can easily be identified.\n",
    "If there is a disagreement, the two annotators can discuss it via a chat window.\n",
    "Similar to SQuAD, three additional answers are collected for each question. However, since the answers influence the flow of the conversation, the next question always depends on the answer to the previous question. For this reason, two different answers to the same question can lead to two different follow-up questions. In order to avoid incoherent discussions, annotators are shown a question that they must answer first. After answering, they are shown the original answer, and they must then confirm that their answer has an identical meaning.\n",
    "\n",
    "Compared to SQuAD 2.0, there is a greater variety of question types in CoQA. While almost half of the questions in the SQuAD start with what, less than a quarter of the questions in the CoQA begin with this token. Another major difference is that questions in CoQA are on average 5.5 words long, compared to an average length of 10.1 in SQuAD. It is also worth mentioning that about 10% of the answers in CoQA are either yes or no, whereas there are no such answers in SQuAD.\n",
    "\n",
    "Like SQuAD, trained models are evaluated using a macro-average F1 score. Models are evaluated separately on the in-domain dataset and the out-of-domain dataset. (Reddy, Chen, and Manning 2018)\n",
    "\n",
    "Humans achieve a F1 score of 89.4 for in-domain and a F1 score of 87.4 for out-of-domain.\n",
    "\n",
    "Currently, the best performing model achieves a F1 score of 91.4 for in-domain and a F1 score of 89.2 for out-of-domain.\n",
    "\n",
    "Examples of CoQA and the leaderboard and can be viewed here:\n",
    "\n",
    "https://stanfordnlp.github.io/coqa/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
