{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103b0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339ab5e",
   "metadata": {},
   "source": [
    "# Concise Implementation of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f771b0",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f6dcc",
   "metadata": {},
   "source": [
    "In PyTorch, the fully connect layer is defined in Linear and LazyLinear classes. The latter allows users to only specify the output dimension while the former additionally asks for how many inputs go into this layer. For simplicity we will use such \"lazy\" layesr whenever we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d322d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(d2l.Module): #@save\n",
    "    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.LazyLinear(1)\n",
    "        self.net.weight.data.normal_(0, 0.01)\n",
    "        self.net.bias.data.fill_(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d9ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression) #@save\n",
    "def forward(self, X):\n",
    "    return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6c195",
   "metadata": {},
   "source": [
    "## Defining the Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb669c",
   "metadata": {},
   "source": [
    "The MSELoss class computes the mean squared error(without the 1/2 factor). By default MSELoss returns the loss over examples. \n",
    "It is faster (and easier to use) than to implement our own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72792d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression) #@save\n",
    "def loss(self, y_hat, y):\n",
    "    fn = nn.MSELoss()\n",
    "    return fn(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b291e44",
   "metadata": {},
   "source": [
    "## Defining the Optimization Algorithm\n",
    "\n",
    "Minibatch SGD is a standard tool for optimizing neural networks and thus Pytorch supports it alongside a number of variations on this algorithm in the optim module. When we instantiate an SGD instance, we specify the paramets to optmize over, obtainable from our model via self.paramets(), and the learning rate(self.lr) requiry by our optmiztion algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c1492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression) #@save\n",
    "def configure_optmizers(self):\n",
    "    return torch.optim.SGD(self.parameters(), self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf929992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
