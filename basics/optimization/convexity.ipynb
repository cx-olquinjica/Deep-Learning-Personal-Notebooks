{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7265a4b8",
   "metadata": {},
   "source": [
    "# Convexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eb4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from mpl_toolkits import mplot3d\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a602a",
   "metadata": {},
   "source": [
    "Convexity plays a vital role in the design of optimization algorithms. This is largely due to the fact that it is much easier to analyze and test algorithms in such a context. In other words, if the algorithm performs poorly even in the convex setting, typically we should not hope to see great results otherwise. Furthermore, even though the optimization problems in deep learning are generally nonconvex, they often exhibit some properties of convex ones near local minima. This can lead to exciting new optimization variants such as: (refer to paper)\n",
    "\n",
    "* https://arxiv.org/abs/1803.05407"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f4a8d",
   "metadata": {},
   "source": [
    "## Definitions: \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
