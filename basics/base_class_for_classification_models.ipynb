{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3fd83af",
   "metadata": {},
   "source": [
    "# The Base Classification Model\n",
    "\n",
    "This section provides a base class for classification models to simplify future code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e814d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf1dbf",
   "metadata": {},
   "source": [
    "The classifier class: in the __validation_step__ we report both the loss value and the classification accuracy on a validation batch. we draw an update for every __num_value_batches__ batches. This has the benefit of generatin the averaged loss and accuracy on the whole validaton data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eca9327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(d2l.Module):  #@save\n",
    "    \"\"\"The base class of classification models.\"\"\"\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b9f48",
   "metadata": {},
   "source": [
    "by default we use SGD optmizer, operating on minibatches, just like as we did in the context of linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed56fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Module)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.SGD(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d338324",
   "metadata": {},
   "source": [
    "Given the predicted probability distribution y_hat, we typically choose the class with the highest predicted probability whenever we must output a hard prediction. Indeed, many applications require that we make a choice. For instance, Gmail must categorize an email into “Primary”, “Social”, “Updates”, “Forums”, or “Spam”. It might estimate probabilities internally, but at the end of the day it has to choose one among the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ccc49e",
   "metadata": {},
   "source": [
    "Accuracy is computed as follows. First, if y_hat is a matrix, we assume that the second dimension stores prediction scores for each class. We use argmax to obtain the predicted class by the index for the largest entry in each row. Then we compare the predicted class with the ground-truth y elementwise. Since the equality operator == is sensitive to data types, we convert y_hat’s data type to match that of y. The result is a tensor containing entries of 0 (false) and 1 (true). Taking the sum yields the number of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ae0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
