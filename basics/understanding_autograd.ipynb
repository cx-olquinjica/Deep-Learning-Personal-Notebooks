{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e027cb6",
   "metadata": {},
   "source": [
    "Best resource to understand Automatic Differentiation: https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb86be",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with torch.autograd\n",
    "\n",
    "When training NN, the most frequently used algorithm is backpropagation. In this algorithm, parameters(model weights) are adjusted according to the __gradient__ of the loss function with respect to the given parameter.\n",
    "\n",
    "What is exactly meant here by __gradient__?\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph.\n",
    "\n",
    "Consider the simplest one-layer network, with input _x_, parameters _w_ ,and _b_, and some _loss function_. It can be defined in Pytorch in the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e14e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss=torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380004aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1.]),\n",
       " tensor([0., 0., 0.]),\n",
       " tensor([[ 0.1120, -0.0213,  0.5249],\n",
       "         [-0.3198, -0.0042, -0.7752],\n",
       "         [ 0.8590,  1.2993, -0.1937],\n",
       "         [-0.1327, -0.9119,  0.6002],\n",
       "         [ 0.4720, -1.5287,  0.9750]], requires_grad=True),\n",
       " tensor([ 0.6012, -0.2897,  2.4452], requires_grad=True),\n",
       " tensor([ 1.5918, -1.4565,  3.5764], grad_fn=<AddBackward0>),\n",
       " tensor(1.8635, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, w, b, z, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750083a3",
   "metadata": {},
   "source": [
    "### Note: You can set the value of requires_grad when creating a tensor, or later by using x.requires_grad(True) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dcd0b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7fe2ebac6340>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fe2ebd0df40>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4658895",
   "metadata": {},
   "source": [
    "In autograd, if any input _Tensor_ of an operation has _requires_grad=True_, the computation will be tracked. After computing the backward pass, a gradient w.r.t this tensor is accumulated into .grad attribute.\n",
    "\n",
    "In the forward phase, the autograd tape will remember all the operations it executed, and in the backward phase, it will replay the operations.\n",
    "\n",
    "If you want to compute the derivatives, you can call .backward() on a _Tensor_. If _Tensor_ is a scalr, you don't need to specify any arguments to backward, however, if it has more elements, you need to specify a _grad_output_ argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4835d",
   "metadata": {},
   "source": [
    "## Second Day on this Automatic Differentiation thing\n",
    "\n",
    "I ain't scared of nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa9803",
   "metadata": {},
   "source": [
    "## Understading Graphs, Automatic Differentiation and Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5adcc42",
   "metadata": {},
   "source": [
    " Automatic Differentiation is a building block of not only PyTorch, but every DL libray out there. In my opinion, PyTorch's automatic differentiation engine, called _Autograd_ is a brilliant tool to understand how automatic differentiation works. This will not only help you understand PyTorch better, but also other DL libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae35d2",
   "metadata": {},
   "source": [
    "Modern neural network architectures can have millions of learnable parameters. From a computational point of view, training a neural network consists of two phases:\n",
    "\n",
    "- A forward pass to compute the value of the loss function. \n",
    "- A backward pass to compute the gradients of the learnable parameters.\n",
    "\n",
    "The forward pass is pretty straightforward. The output of one layer is the input to the next and so forth. \n",
    "\n",
    "Backward pass is a bit more complicated since it requires us to use the chain rule to compute the gradients of weights with respect to the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf7c7c",
   "metadata": {},
   "source": [
    "The nodes on the Computational Graph are basically __operators__. These operators are basically the mathematical operators except for one case, wehre we need to represent creation of user-defined variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5e2bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tsr = torch.Tensor(3, 5)\n",
    "tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be001063",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mones()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'torch'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6f52d65",
   "metadata": {},
   "source": [
    "While initiating the variables that are going to be part of the computation, one should set the requires_grad = True in order to make sure it appears in the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ab7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn((3,3), requires_grad = True)\n",
    "t2 = torch.FloatTensor(3,3) # we will use requires_grad outside the initialization\n",
    "t2.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5489f3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0759,  1.0211,  0.0208],\n",
       "        [-1.6086, -0.2280, -1.8427],\n",
       "        [-0.3586, -0.8975,  0.4918]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730b40b",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "requires_grad is __contagious__. It meas that when a _Tensor_ is created by operating on other _Tensor_, the requires_grad of the resultant Tensor would be set __True__ given at least one of the tensors used for creating has it's requires_grad set to True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c8ed3",
   "metadata": {},
   "source": [
    " Each _Tensor_ has an attribute called __grad_fn__, which refers to the mathematical operator that create the variable. Notice that it is only available if requires_grad is set to True, otherwise it will return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c14a9862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1827, -0.7592,  0.0770],\n",
       "        [ 1.1466, -1.2832, -0.3279],\n",
       "        [ 0.7889, -0.2127, -0.4810]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((3,3), requires_grad = True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f42b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn((3, 3), requires_grad = True)\n",
    "w2 = torch.randn((3, 3),requires_grad = True)\n",
    "w3 = torch.randn((3, 3), requires_grad = True)\n",
    "w4 = torch.rand((3, 3), requires_grad = True)\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3*b + w4*c \n",
    "L = 10 - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2488232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grad fn for a is, None\n",
      "The grad fn for d is, <AddBackward0 object at 0x7f7e7ffd2ee0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"The grad fn for a is, {a.grad_fn}\")\n",
    "print(f\"The grad fn for d is, {d.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af64e86",
   "metadata": {},
   "source": [
    "# Very important explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd4943",
   "metadata": {},
   "source": [
    "In our example, where d = f(w3b, w4c), d's grad function would be the addition operation as shown by the computational graph. However, if our Tensor is a leaf node (initialized by the user, then the grad_fn is None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4802c56",
   "metadata": {},
   "source": [
    "One can use the member function __is_leaf__ to determine whether a variable is a leaf _Tensor_ or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c3061eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x7f7e7ffd2880>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1ee04",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4570a",
   "metadata": {},
   "source": [
    " All mathematical operations in PyTorch are implemented by the __torch.nn.Autograd__ Function class. This class has two important member functions we need to look at. \n",
    " \n",
    " The first is __forward__ function, which simply computes the output using it's inputs (i.e when calculating the loss function). \n",
    " \n",
    " The __backward__ function takes the incoming gradient coming from the part of the network in front of it. As you can see, the gradient to be backpropagated from a function _f_ is basically the __gradient that is backpropagated to _f_ from the layers in front of it multiplied by the local gradient of the output of _f_ with respect to it's inputs.__ This is exactly what the __backward__ function does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca6d117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gradient of L wrt to d in stored in grad attribute of the d:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wt/_gch94hd6mn__nbcg4r137r80000gn/T/ipykernel_1359/2114103027.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)\n",
      "  d.grad\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example here using the backward function on d, which\n",
    "takes the gradient of d'l/d'd, and we can get this value from \n",
    "d.grad\"\"\"\n",
    "print(\" Gradient of L wrt to d in stored in grad attribute of the d:\")\n",
    "d.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ce733",
   "metadata": {},
   "source": [
    "The code above is basically telling me that __we only find the gradient of a node that is a leaf!!!!!!!!!!!!!!!!__ that answers the question I had yesterday regarding the use of grad. Now proving my point with the leafs  _w1, w2, w3, w4_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc70aa",
   "metadata": {},
   "source": [
    "# WAIT GENIUS!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ed289",
   "metadata": {},
   "source": [
    "1) In order to compute derivatives in our NN, we generally call __backward__ on the __Tensor__ representing our loss. \n",
    "2) We backtrack through the graph starting from the node representing the __grad_fn__ of our loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf7ad2",
   "metadata": {},
   "source": [
    "In order to compute derivatives in our neural network, we generally call backward on the Tensor representing our loss. Then, we backtrack through the graph starting from node representing the grad_fn of our loss.\n",
    "\n",
    "As described above, the backward function is recursively called through out the graph as we backtrack. Once, we reach a leaf node, since the grad_fn is None, but stop backtracking through that path.\n",
    "\n",
    "One thing to note here is that PyTorch gives an error if you call backward() on vector-valued Tensor. This means you can only call backward on a scalar valued Tensor. In our example, if we assume a to be a vector valued Tensor, and call backward on L, it will throw up an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cec79fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting the existing data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f7e7b6e76a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Overwriting the existing data:\")\n",
    "\n",
    "\n",
    "a = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)\n",
    "w2 = torch.randn((3,3), requires_grad = True)\n",
    "w3 = torch.randn((3,3), requires_grad = True)\n",
    "w4 = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "b = w1*a \n",
    "c = w2*a\n",
    "\n",
    "d = w3*b + w4*c \n",
    "\n",
    "L = (10 - d)\n",
    "d.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "988cd0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RsubBackward1 at 0x7f7e806847f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ecfb46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5748575c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5336451b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/autograd/__init__.py:190\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    189\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 190\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/autograd/__init__.py:85\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46de32",
   "metadata": {},
   "source": [
    "# Note: \n",
    "\n",
    "This is because gradients can be computed with respect to scalar values by definition. You can't exactly differentiate a vector with respect to another vector. The mathematical entity used for such cases is called a Jacobian, the discussion of which is beyond the scope of this article.\n",
    "\n",
    "There are two ways to overcome this.\n",
    "\n",
    "If you just make a small change in the above code setting L to be the sum of all the errors, our problem will be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c113b4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting L\n"
     ]
    }
   ],
   "source": [
    "print(\"Overwriting L\")\n",
    "L = (10 -d).sum()\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "062d52f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0447, -0.1020,  0.2793],\n",
       "        [-0.0362,  0.3686,  0.0630],\n",
       "        [-0.4692, -0.6442,  1.2812]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cea8fe89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0232, -0.2550, -2.9517],\n",
       "        [-0.2709, -1.1645, -0.0170],\n",
       "        [ 2.1593, -0.0626, -1.5296]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97df1e",
   "metadata": {},
   "source": [
    "# How are PyTorch's graphs different from TensorFlow graphs?\n",
    "\n",
    "PyTorch creates something called a Dynamic Computation Graph, which means that the graph is generated on the fly.\n",
    "\n",
    "Until the forward function of a Variable is called, there exists no node for the Tensor (it’s grad_fn) in the graph.\n",
    "\n",
    "The graph is created as a result of forward function of many Tensors being invoked. Only then, the buffers for the non-leaf nodes allocated for the graph and intermediate values (used for computing gradients later.  When you call backward, as the gradients are computed, these buffers (for non-leaf variables) are essentially freed, and the graph is destroyed ( In a sense, you can't backpropagate through it since the buffers holding values to compute the gradients are gone).\n",
    "\n",
    "Next time, you will call forward on the same set of tensors, the leaf node buffers from the previous run will be shared, while the non-leaf nodes buffers will be created again.\n",
    "\n",
    "If you call backward more than once on a graph with non-leaf nodes, you'll be met with the following error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516fa3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
