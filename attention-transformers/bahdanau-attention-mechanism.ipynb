{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b32b12",
   "metadata": {},
   "source": [
    "# The Bahdanau Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66f400",
   "metadata": {},
   "source": [
    "When talking about machine translation using a RNN in the form of an encoder-decodre architecture for sequence to sequence learning. Specifically, the RNN encoder transfomrs a variable-lenght sequence into a _fixed-shape_ context variable. Then, the RNN decoder generates the output (target) sequence token by token based on the generated tokens and the context variable.\n",
    "\n",
    "Conventionally, in an RNN all relevant information about a source seauence is translated into some internal _fixed-dimensional state_ representation by the encoder. It is this very state that is used by the decoder as teh complete exclusive source of informaton to generate the tranlated sequence. In other words, the seq2seq mechanism __treats the intermediate state as a sufficient statistic of whatever string might have served as input.__\n",
    "\n",
    "While this is quite reasonable for short sequences, it is clear that is is infeasible for long sequences, such as a book chpater or even just a very logn sentence. After all, after a while there will simply \"__not be enough space__\" in the intermediate representation to store all that is important in the source sequence. Consequently the decoder will fail to translate long and complex sentences. One of the first to encounter Graves, et al 2013 when they tried to design an RNN to generate handwritten text. Since the source text has arbitraty length they designed a differentiable attention model to align text characters with the much longer pen trace, where the alignment moves only in one direction. This, in turn, draws on decoding algorithms in speech recognition.\n",
    "\n",
    "Inspired by the _idea of learning to align_, Bahdanau et al (2014) proposed a differentiable attention model without the unidirectional alignment limitation. When predicting a token, if not all the input tokens are relevant, the model aligns (or attends) only to parts of the inputs sequences that are deemed relevant to the current prediction. This is then used to update the current state before generating the next token. While quite innocuous in its description, this Bahdanau attention mechanism has arguably turned into one of the most influential ideas of the past decade in deep learning, giving rise to _Transformers_ and many related new architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50670e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87e16d",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The key idea is that instead of keeping the state, i.e., the context variable c summarizing the source sentence as fixed, we dynamically update it, as a function of both the original text (encoder hidden states ht) and the text that was already generated (decoder hidden states).\n",
    "\n",
    "In this way, _c_ is update after any decoding time _t_. \n",
    "The attention weight using teh additive attention scoring function.\n",
    "\n",
    "### Defining the Decoder with Attention\n",
    "\n",
    "To implement the RNN encoder-decoder with attention, we only need to redefine the decoder (omitting the generated symbols from the attention function simplifies the design). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3331b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(d2l.Decoder):  #@save\n",
    "    \"\"\"The base attention-based decoder interface.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7a1aa",
   "metadata": {},
   "source": [
    "We need to implement the RNN decoder in the Seq2SeqAttentionDecoder class. The state of the decoder is initialized with (i) the hidden states of the last layer of the encoder at all time steps, used as keys and values for attention; (ii) the hidden state of the encoder at all layers at the final time step. This serves to initialize the hidden state of the decoder; and (iii) the valid length of the encoder, to exclude the padding tokens in attention pooling. At each decoding time step, the hidden state of the last layer of the decoder, obtained at the previous time step, is used as the query of the attention mechanism. Both the output of the attention mechanism and the input embedding are concatenated to serve as the input of the RNN decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963e01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "            dropout=dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        self.apply(d2l.init_seq2seq)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens):\n",
    "        # Shape of outputs: (num_steps, batch_size, num_hiddens).\n",
    "        # Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # Shape of enc_outputs: (batch_size, num_steps, num_hiddens).\n",
    "        # Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n",
    "        enc_outputs, hidden_state, enc_valid_lens = state\n",
    "        # Shape of the output X: (num_steps, batch_size, embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        outputs, self._attention_weights = [], []\n",
    "        for x in X:\n",
    "            # Shape of query: (batch_size, 1, num_hiddens)\n",
    "            query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
    "            # Shape of context: (batch_size, 1, num_hiddens)\n",
    "            context = self.attention(\n",
    "                query, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "            # Concatenate on the feature dimension\n",
    "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
    "            # Reshape x as (1, batch_size, embed_size + num_hiddens)\n",
    "            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
    "            outputs.append(out)\n",
    "            self._attention_weights.append(self.attention.attention_weights)\n",
    "        # After fully connected layer transformation, shape of outputs:\n",
    "        # (num_steps, batch_size, vocab_size)\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
    "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n",
    "                                          enc_valid_lens]\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dee650",
   "metadata": {},
   "source": [
    "In the following, we test the implemented decoder with attention using a minibatch of 4 sequences, each of which are 7 time steps long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4000cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size, num_steps = 4, 7\n",
    "encoder = d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "decoder = Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens,\n",
    "                                  num_layers)\n",
    "X = torch.zeros((batch_size, num_steps), dtype=torch.long)\n",
    "state = decoder.init_state(encoder(X), None)\n",
    "output, state = decoder(X, state)\n",
    "d2l.check_shape(output, (batch_size, num_steps, vocab_size))\n",
    "d2l.check_shape(state[0], (batch_size, num_steps, num_hiddens))\n",
    "d2l.check_shape(state[1][0], (batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3581c8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
