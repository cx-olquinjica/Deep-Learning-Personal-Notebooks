{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102b81ab",
   "metadata": {},
   "source": [
    "# Attention Mechanisms and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fe40c",
   "metadata": {},
   "source": [
    "The earliest years of the deep learning boom were driven primarily by results produced using the multilayer perceptron, convolutional network, and recurrent network architectures. Remarkably, the model architectures that underpinned many of deep learning's breakthrouhgs in the 2010s had changed remarkably little relative to their antecendentes despite the lapse of nearly 30 years. While plenty of new methodological innovations made their way into most practitioner's toolkits--__ReLU activations, residual layers, batch normalization, dropout, and adaptive learning rate schedules__ come to mind the core underlying architetures were clearly recognizable as scaled-up implementations of classic ideas. Despite thousands of papers proposing alternative ideas, models resembling classical convolutional neural networks retained _stae of the art_ status in computer vision and models resembling Sepp Hochreiter's original design for the LSTM recurrent neural network, dominated most applications in natural language processing. Arguably, to that point, the rapid emergence of deep learning appeared to be primarily attributable to shifts in the available computational resources (due to innovaitons in parallel computing with GPUs) and the availability of massive data resources (due to cheap storage and internet services). While these factors may indeed remain the primary drivers behind this technology's increasing power we are also witnessing, at long last, a sea change in the landscape of dominante architectures.\n",
    "\n",
    "\n",
    "At the present moment, the dominant models for nearly all natural language processing tasks are _based on the Transformer architecture_. Given any new task in natural language processing:\n",
    "1) the default first-pass approach is to grab a large Transformer-based pretrained model, (e.g., BERT (Devlin et al., 2018), ELECTRA (Clark et al., 2020), RoBERTa (Liu et al., 2019), or Longformer (Beltagy et al., 2020)) \n",
    "2) adapting the output layers as necessary, \n",
    "3) and fine-tuning the model on the available data for the downstream task.\n",
    "\n",
    "If you have been paying attention to the last few years of breathless news coverage centered on OpenAIâ€™s large language models, then you have been tracking a conversation centered on the GPT-2 and GPT-3 Transformer-based models (Brown et al., 2020, Radford et al., 2019). \n",
    "\n",
    "#### Transformers implemented in different areas:\n",
    "* __Computer Vision__: The vision Transformer has emerged as a default model for diverse vision tasks, including image recognition, object detection, semantic segmentation, and superresolution (Dosovitskiy et al., 2021, Liu et al., 2021). \n",
    "\n",
    "* __Speech Recognition__: Transformers also showed up as competitive methods for speech recognition (Gulati et al., 2020)\n",
    "\n",
    "* __Reinforcement Learning__: Reinforcement learning (Chen et al., 2021)\n",
    "\n",
    "* __Graph Neural Networkds__: Graph neural networks (Dwivedi and Bresson, 2020).\n",
    "\n",
    "__The core idea behind the Transformer model is the attention mechanism, an innovation that was originally envisioned as an enhancement for encoder-decoder RNNs applied to sequence-to-sequence applications, like machine translations (Bahdanau et al., 2014).__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
