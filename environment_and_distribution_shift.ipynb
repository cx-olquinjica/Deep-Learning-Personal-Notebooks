{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7845ba85",
   "metadata": {},
   "source": [
    "# 4.7. Environment and Distribution Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaab657",
   "metadata": {},
   "source": [
    "Sometimes when building machine learning applications we never stop to contemplate either wehre data comes from in the first place or what we plan to ultimately do with the outputs from our models. Too often, machine learning developers in possession of data rush to developt models without pausing to consider these fundamental issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd8224",
   "metadata": {},
   "source": [
    "Many failed machine learning deployments can be traced back to this pattern. Sometimes models appear to perfom marvelously as meausred by test set accuracy but fail catastrophically in deployment when the distribution of data suddenly shifts. More insidiously, sometimes the very deployment of a model can be catalyst that perturbs the data distribution. Say, for example, that we trained a model to predict who will replay vs. defaut on a lona, finding an applicant's choice of footwear as associated with the risk of default(Oxfords indicate repayment, sneakers indicate default). We might be inclined to thereafter grant loans to all applicants wearing Oxfords and to deny all applicants wearing sneakers.<br>\n",
    "\n",
    "In this case, our ill-considered leap from patter recognition to decision-making and our failure to critically consider the environment might have disastros consequences. <br>\n",
    "\n",
    "While we cannot possibly give these topics a complet treatment in one section, we aim here to expose some common concerns, and to stimulate the critical thinking required to detect these situations early, mitigace damage, and use machine learning responsibly. Some of the solutions are simple(ask for the \"right\" data), some are technically difficult(implement reinforcement learning system), and others require that we step outside the real of statistical prediction altogether and grapple with difficult philosophical questions concerning the ethical application of algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b7c6d",
   "metadata": {},
   "source": [
    "## 4.7.1. Types of Distribution Shift "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b6755",
   "metadata": {},
   "source": [
    "- Covariate Shift\n",
    "- Label Shift \n",
    "- Concept Shift "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8e958",
   "metadata": {},
   "source": [
    "## 4.7.2. Examples of Distribution Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c695c90",
   "metadata": {},
   "source": [
    "### 4.7.2.1 Medical Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38f92d",
   "metadata": {},
   "source": [
    "Imagine that you want to design an algorithm to detect cancer. You collect data from healthy and sick people and you train your algorithm. It works fine, giving you high accuracy and you conclude that you are ready for a successful career in medical diagnostics. Not so fast.<br>\n",
    "\n",
    "The distributions that gave rise to the training data and those you will encounter in the wild might differ considerably. This happened to an unfortunate startup that some of us (authors) worked with years ago. They were developing a blood test for a disease that predominantly affects older men and hoped to study it using blood samples that they had collected from patients. However, it is considerably more difficult to obtain blood samples from healthy men than sick patients already in the system. To compensate, the startup solicited blood donations from students on a university campus to serve as healthy controls in developing their test. Then they asked whether we could help them to build a classifier for detecting the disease.<br>\n",
    "\n",
    "As we explained to them, it would indeed be easy to distinguish between the healthy and sick cohorts with near-perfect accuracy. However, that is because the test subjects differed in age, hormone levels, physical activity, diet, alcohol consumption, and many more factors unrelated to the disease. This was unlikely to be the case with real patients. Due to their sampling procedure, we could expect to encounter extreme __covariate shift__. Moreover, this case was unlikely to be correctable via conventional methods. In short, they wasted a significant sum of money."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281b733",
   "metadata": {},
   "source": [
    "### 4.7.2.2. Self-Driving Cars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d935f5",
   "metadata": {},
   "source": [
    "Say a company wanted to leverage machine learning for developing self-driving cars. One key component here is a roadside detector. Since real annotated data is expensive to get, they had the (smart and questionable) idea to use synthetic data from a game rendering engine as additional training data. This worked really well on “test data” drawn from the rendering engine. Alas, inside a real car it was a disaster. As it turned out, the roadside had been rendered with a very simplistic texture. More importantly, all the roadside had been rendered with the same texture and the roadside detector learned about this “feature” very quickly.<br>\n",
    "\n",
    "A similar thing happened to the US Army when they first tried to detect tanks in the forest. They took aerial photographs of the forest without tanks, then drove the tanks into the forest and took another set of pictures. The classifier appeared to work perfectly. Unfortunately, it had merely learned how to distinguish trees with shadows from trees without shadows—the first set of pictures was taken in the early morning, the second set at noon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac51f0",
   "metadata": {},
   "source": [
    "### 4.7.2.3. Nonstationary Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c5de31",
   "metadata": {},
   "source": [
    "A much more subtle situation arises when the distribution changes slowly (also known as nonstationary distribution) and the model is not updated adequately. Below are some typical case:<br>\n",
    "\n",
    "- We train a computational advertising model and then fail to update it frequently (e.g., we forget to incorporate that an obscure new device called an iPad was just launched).<br>\n",
    "\n",
    "- We build a spam filter. It works well at detecting all spam that we have seen so far. But then the spammers wisen up and craft new messages that look unlike anything we have seen before.<br>\n",
    "\n",
    "- We build a product recommendation system. It works throughout the winter but then continues to recommend Santa hats long after Christmas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be7254",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
