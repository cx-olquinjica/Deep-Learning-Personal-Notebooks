{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95f0983",
   "metadata": {},
   "source": [
    "# CNN Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbaf32f",
   "metadata": {},
   "source": [
    "## LeNet:\n",
    "### Main idea:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3554229",
   "metadata": {},
   "source": [
    "At a high level, LeNet-5 consists of two parts:\n",
    "\n",
    "* A convolutional encoder consisting of two convolutional layers.\n",
    "* A dense block consisting of three fully connected layers. \n",
    "\n",
    "\n",
    "Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcda954",
   "metadata": {},
   "source": [
    "__Convolutional Layer__: \n",
    "\n",
    "* Each convolutional layer uses a 5 x 5 kernel\n",
    "* Sigmoid Activation function\n",
    "* Average pooling.\n",
    "\n",
    "In resume: \n",
    "    5 x 5 kernel -> Sigmoid activation function -> 2 x 2 average pooling (stride 2)\n",
    "\n",
    "### Number of output channels: \n",
    "* 1st convolutional layer has 6 output channel\n",
    "* 2nd                         16          \n",
    "\n",
    "The convolutional block emits an output with shape given by (batch size, number of channel, height, width)\n",
    "\n",
    "### Question: Why the number of output channel increases as the data moves from one convolutional layer to the other?\n",
    "\n",
    "#### Answer: \n",
    "These layers map spatially arranged inputs to a number of two-dimensional feature maps, typically increasing the number of channels. \n",
    "\n",
    "### Dense Block: \n",
    "\n",
    "In order to pass output from the convolutional block to the dense block, we must flatten each example in the minibatch. In other words, we take this four-dimensional input and transform it into the two-dimensional input expected by fully connected layers: as a reminder, the two-dimensional representation taht we desire uses the first dimension to index examples n the minibatch and the second to give the flat vector representation of each example. \n",
    "\n",
    "LeNet dense block has three fully connected layers, with 120, 84, and 10 outputs, respectively.\n",
    "\n",
    "### Problems: \n",
    "\n",
    "* Small dataset\n",
    "* Not enough computational power\n",
    "* Low resolution images 28 x 28\n",
    "* No key tricks for training neural networks (parameter initialization)\n",
    "* No clever variants for SGD\n",
    "* Non-squashing activation function\n",
    "* Effective regularization techniques were still missing\n",
    "\n",
    "Although LeNet-5 achieved good results on early small datasets, the performance and feasibility of training CNNs on larger, more realistic datasets had yet to be established.\n",
    "\n",
    "Even though the inputs to convolutional networks consist of raw or lightly-processed (e.g., by centering) piexel values, practicionaers would never feed _raw_ pixels into traditional models. Instead, typical computer vision pipelines consisted of manually engineering feature extraction pipelines, such as SIFT, and SURF. Rather than _learning the features_, the features were crafted. Most of the progress came from having more clever ideas for feature extraction on the one hand and deep insight into geometry on the other hand.The learning algorithm was oftern considered an afterthought.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24aea3",
   "metadata": {},
   "source": [
    "## AlexNet:\n",
    "### Main idea:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6f1a9",
   "metadata": {},
   "source": [
    "Althoug feature extraction tools ruled the roost prior to the introduction of AlexNet, another group of reseachers believed that features themselves ought to be learned instead of manually engineered. Moreover, they beleved that to be reasonably complex, the features ought to be hierarchically composed with multiple jointly learned layers, each with learnable parameters. In the case of image, the lowest layers might come to detect edges, colors, and textures, in analogy to how the visual system in animal processes its input. \n",
    "\n",
    "AlexNet (2012) and its precursor LeNet (1995) share many architectural elements. This begs the question: why did it take so long? A key difference is that over the past two decades, the amount of data and computing power available had increased significantly. As such AlexNet was much larger: it was trained on much more data, and on much faster GPUs, compared to the CPUs available in 1995."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10273dc",
   "metadata": {},
   "source": [
    "### LeNet-5 vs AlexNet:\n",
    "\n",
    "* AlexNet is much deeper than the comparatively small LeNet5\n",
    "* AlexNet consists of eight layers:\n",
    "    * 5 convolutional layer\n",
    "    * 2 fully connected hidden layers\n",
    "    * 1 fully connected output layer\n",
    "* AlexNet controls the model complexity of the fully connected layer by dropout, while LeNet only uses weight decay.\n",
    "* AlexNet used the ReLU instead of the Sigmoid as its activation function. \n",
    "\n",
    "On the one hand, the computation of the ReLU activation function is simple. For example it does not have the exponentiation operation found in the sigmoid activation function. On the other hand, ReLU makes model training easier when using different parameter initialization methods.This is because, when the output of the sigmoid activation function is very close to 0 or 1, the gradient of these regions is almost 0, so that backpropagation cannot continue to update some of the model parameters. In contrast, the gradient of the ReLU activation function in the positive interval is always 1. Therefore, if the model parameters are not properly initialized, the sigmoid function may obtain a gradient of almost 0 in the positive interval, so that the model cannot be effectively trained.\n",
    "\n",
    "__Convolutional Layer__:\n",
    "\n",
    "* 1st layer: 11 x 11 kernel (since the images in ImageNet are eight times higher and wider than MNIST images.\n",
    "* 2nd layer: 5 x 5\n",
    "* 3rd layer: 3 x 3\n",
    "\n",
    "After the 1st, 2nd, and 5th convolutional layers, the network adds max_pooling layers with:\n",
    "\n",
    "* 3 x 3 window shape and stride of 2\n",
    "\n",
    "After the last convolutional layer, there are two __huge__ fully connected layers with 4096 outputs. These layers require nearly 1GB model parameters. \n",
    "\n",
    "AlexNetâ€™s structure bears a striking resemblance to LeNet, with a number of critical improvements, both for accuracy (dropout) and for ease of training (ReLU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a92f8",
   "metadata": {},
   "source": [
    "### Problems: \n",
    "\n",
    "Reviewing the architecture, we see that AlexNet has an Achilles heel when it comes to _efficiency_: \n",
    "\n",
    "* The last two layers require matrices of size 6400 x 4096 and 4096 x 4095, respectively. \n",
    "\n",
    "\n",
    "This corresponds to 164 MB of memory and 81 MLFLOPs of computation, both of which are a nontrivial outlay, especially on smaller devices, such as mobile phones. This is one of the reasons why AlexNet has been surpassed by much more effective architectures. \n",
    "\n",
    "Note that even though the number of parameters by far exceeds the amount of training data in our experiments (the last two layers have more than 40 million parameters, trained on a datasets of 60 thousand images), there is hardly any overfitting: training and validation loss are virtually identical throughout training. This is due to the improved regularization, such as Dropout, inherent in modern deep network designs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8277719",
   "metadata": {},
   "source": [
    "## VGG:\n",
    "### Main idea:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e0065",
   "metadata": {},
   "source": [
    "The basic building block of CNNs is a sequence of the following:\n",
    "\n",
    "* a convolutional layer with padding to maintain the resolution\n",
    "* a nonlinearity such as ReLU\n",
    "* a pooling layer such as max-pooling to reduce the resolution\n",
    "\n",
    "However, one of the problems with this approach is that the spatial resolution decreases quite rapidly. In particular, this imposes a hard limit of lg d convolutional layers on the network before all dimensions (d) are used up. For instance, in the case of ImageNet, it would be impossible to have more than 8 convolutional layers in this way.\n",
    "\n",
    "The key of the paper introducing VGG was to use _multiple convolutions in between downsampling via max-pooling in the form of a block_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbda468",
   "metadata": {},
   "source": [
    " They were primarily interested in whether deep or wide networks perform better. For instance, the successive application of two  convolutions touches the same pixels as a single  convolution does. At the same time, the latter uses approximately as many parameters () as three  convolutions do (). In a rather detailed analysis they showed that deep and narrow networks significantly outperform their shallow counterparts. This set deep learning on a quest for ever deeper networks with over 100 layers for typical applications. Stacking  convolutions has become a gold standard in later deep networks (a design decision only to be revisited recently by Liu et al. (2022)). Consequently, fast implementations for small convolutions have become a staple on GPUs (Lavin and Gray, 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b053a28",
   "metadata": {},
   "source": [
    "### VGG Architecture: \n",
    "\n",
    "A VGG block consists of a sequence of convolutions with:\n",
    "\n",
    "* 3 x 3 kernels with padding 1 (keeping height and width)\n",
    "* 2 x 2 max-pooling layer with stride of 2 (halving height and width after each block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f2e1e",
   "metadata": {},
   "source": [
    "The original VGG network had 5 convolutional blocks, among which the first two have one convolutional layer each and the latter three contain two convolutional layers each. The first block has 64 output channels and each subsequent block doubles the number of output channels, until that number reaches 512. Since this network uses 8 convolutional layers and 3 fully connected layers, it is often called VGG-11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64138fe3",
   "metadata": {},
   "source": [
    "### Problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434be9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
