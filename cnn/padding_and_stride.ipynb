{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded5d0bb",
   "metadata": {},
   "source": [
    "# Padding and Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c07f39",
   "metadata": {},
   "source": [
    "Padding and Stride offer us more options to customize the output of our convolutions. As a motivation, note that since kernels generally have width and height greater than 1, after applying many sucessive convolutions, we tend to wind up with outputs that are considerably smaller than our input. If we start with a 240 x 240 pixel image, 10 layers of 5 x 5 convolutions reduce the image to 200 x 200 pixels, slicing off 30% of the image with it obliterating any interesting information on the boundaries of the original image. __Padding__ is the most popular tool for handlign this issue. In other cases, we may want to reduce the dimensionality drastically, e.g., if we fidn the original input resolution to be unwieldly. __Strided Convolutions__ are a popular technique that can help in these instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe1f6d",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702722f1",
   "metadata": {},
   "source": [
    "Since we typically use small kernels, for any given convolution, we might only lose a few pixels, but this can add up as we apply many sucessive convolutional layers. One straightforward soltion to this problem is to add extra pixels of filler around the boundary of our input image, thus increasing the effective size of the image. Typically, we set the values of the extra pixel to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300f5aa",
   "metadata": {},
   "source": [
    "### Mathematical Relationship between the padding and the output or the feauture map\n",
    "\n",
    "\n",
    "In In general, if we add a total of ph rows of padding (roughly half on top and half on bottom) and a total of pw columns of padding (roughly half on the left and half on the right), the output shape will be:\n",
    "\n",
    "            (nh - kh + ph + 1) x (nw - kw + pw + 1)\n",
    "\n",
    "In many cases, we will want to set ph = kh -1 and pw = kw - 1 to give the input and output the same height and width. This will make it easier to predict the output shape of each layer when constructing the network. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46e290",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "CNNs commonly use convolution kernels with odd height and width values, such as 1, 3, 5, or 7. Choosing odd kernel sizes has the benefit that we can preserve the dimensionality while padding with the same number of rows on top and bottom, and the same number of columns on left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75441427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# We define a helper function to calculate convolutions. It initializes the\n",
    "# convolutional layer weights and performs corresponding dimensionality\n",
    "# elevations and reductions on the input and output\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # (1, 1) indicates that batch size and the number of channels are both 1\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # Strip the first two dimensions: examples and channels\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "# 1 row and column is padded on either side, so a total of 2 rows or columns\n",
    "# are added\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f89e2",
   "metadata": {},
   "source": [
    "## Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d30a6",
   "metadata": {},
   "source": [
    "When computing the cross-correlation, we start with the convolution window at the upper-left corner of the input tensor, and then slide it over all locations both down and to the right. In the previous examples, we defaulted to sliding one element at a time. However, sometimes, either for computational efficiency or because we wish to downsample, we move our window more than one element at a time, skipping the intermediate locations. This is particularly useful if the convolution kernel is large since it captures a large area of the underlying image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0c5ee",
   "metadata": {},
   "source": [
    "We refer to the number of rows and columns traversed per slide as stride. So far, we have used strides of 1, both for height and width. Sometimes, we may want to use a larger stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c30bb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b2390c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879f23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
